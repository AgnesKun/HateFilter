{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate Filter\n",
    "I hope all SNS companies (FaceBook, Twitter, Line, etc.) implemnts Hate filter in every languages.  I don't have enough training sets.  But I wanted to prove this works in Japanese."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Japanese word extracter using MeCab (Japanese Morphological Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class WordDividor:\n",
    "    INDEX_CATEGORY = 0\n",
    "    INDEX_ROOT_FORM = 6\n",
    "    # \"Noun\", \"Verb\", \"Adjective\", \"Adverb\", \"Adonominal\", \"Emotive Verv\"\n",
    "    TARGET_CATEGORIES = [\"名詞\", \" 動詞\",  \"形容詞\", \"副詞\", \"連体詞\", \"感動詞\"]\n",
    "\n",
    "    def __init__(self, dictionary=\"mecabrc\"):\n",
    "        self.dictionary = dictionary\n",
    "        self.tagger = MeCab.Tagger(self.dictionary)\n",
    "\n",
    "    def extract_words(self, text):\n",
    "        if not text:\n",
    "            return []\n",
    "\n",
    "        words = []\n",
    "\n",
    "        node = self.tagger.parseToNode(text)\n",
    "        while node:\n",
    "            features = node.feature.split(',')\n",
    "\n",
    "            if features[self.INDEX_CATEGORY] in self.TARGET_CATEGORIES:\n",
    "                #print(str(features))\n",
    "                if features[self.INDEX_ROOT_FORM] == \"*\":\n",
    "                    words.append(node.surface)\n",
    "                else:\n",
    "                    # prefer root form\n",
    "                    words.append(features[self.INDEX_ROOT_FORM])\n",
    "\n",
    "            node = node.next\n",
    "\n",
    "        return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading senteces from file\n",
    "Not like sentiments analysis, one training set has only one sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import shutil\n",
    "from collections import namedtuple\n",
    "from os import environ, listdir, makedirs\n",
    "from os.path import dirname, exists, expanduser, isdir, join, splitext\n",
    "import hashlib\n",
    "\n",
    "from sklearn.datasets.base import Bunch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_sentence_files(container_path, description=None, categories=None,\n",
    "               encoding=None,\n",
    "               decode_error='strict', random_state=0):\n",
    "    \"\"\"Load text files with categories as subfolder names.\n",
    "    Individual samples are assumed to be files stored a two levels folder\n",
    "    structure such as the following:\n",
    "        container_folder/\n",
    "            category_1_folder/\n",
    "                file_1.txt\n",
    "                file_2.txt\n",
    "                ...\n",
    "                file_42.txt\n",
    "            category_2_folder/\n",
    "                file_43.txt\n",
    "                file_44.txt\n",
    "                ...\n",
    "    The folder names are used as supervised signal label names. The individual\n",
    "    file names are not important.\n",
    "    This function does not try to extract features into a numpy array or scipy\n",
    "    sparse matrix. In addition, if load_content is false it does not try to\n",
    "    load the files in memory.\n",
    "    To use text files in a scikit-learn classification or clustering algorithm,\n",
    "    you will need to use the `sklearn.feature_extraction.text` module to build\n",
    "    a feature extraction transformer that suits your problem.\n",
    "    If you set load_content=True, you should also specify the encoding of the\n",
    "    text using the 'encoding' parameter. For many modern text files, 'utf-8'\n",
    "    will be the correct encoding. If you leave encoding equal to None, then the\n",
    "    content will be made of bytes instead of Unicode, and you will not be able\n",
    "    to use most functions in `sklearn.feature_extraction.text`.\n",
    "    Similar feature extractors should be built for other kind of unstructured\n",
    "    data input such as images, audio, video, ...\n",
    "    Read more in the :ref:`User Guide <datasets>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    container_path : string or unicode\n",
    "        Path to the main folder holding one subfolder per category\n",
    "    description : string or unicode, optional (default=None)\n",
    "        A paragraph describing the characteristic of the dataset: its source,\n",
    "        reference, etc.\n",
    "    categories : A collection of strings or None, optional (default=None)\n",
    "        If None (default), load all the categories. If not None, list of\n",
    "        category names to load (other categories ignored).\n",
    "    encoding : string or None (default is None)\n",
    "        If None, do not try to decode the content of the files (e.g. for images\n",
    "        or other non-text content). If not None, encoding to use to decode text\n",
    "        files to Unicode if load_content is True.\n",
    "    decode_error : {'strict', 'ignore', 'replace'}, optional\n",
    "        Instruction on what to do if a byte sequence is given to analyze that\n",
    "        contains characters not of the given `encoding`. Passed as keyword\n",
    "        argument 'errors' to bytes.decode.\n",
    "    random_state : int, RandomState instance or None, optional (default=0)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "    Returns\n",
    "    -------\n",
    "    data : Bunch\n",
    "        Dictionary-like object, the interesting attributes are: either\n",
    "        data, the raw text data to learn, or 'filenames', the files\n",
    "        holding it, 'target', the classification labels (integer index),\n",
    "        'target_names', the meaning of the labels, and 'DESCR', the full\n",
    "        description of the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    folders = [f for f in sorted(listdir(container_path))\n",
    "               if isdir(join(container_path, f))]\n",
    "\n",
    "    if categories is not None:\n",
    "        folders = [f for f in folders if f in categories]\n",
    "\n",
    "    target = []\n",
    "    target_names = []\n",
    "    data = []\n",
    "    files = []\n",
    "    for label, folder in enumerate(folders):\n",
    "        target_names.append(folder)\n",
    "        folder_path = join(container_path, folder)\n",
    "        documents = [join(folder_path, d)\n",
    "                     for d in sorted(listdir(folder_path))]\n",
    "        \n",
    "        for filename in documents:\n",
    "            with open(filename, 'rb') as f:\n",
    "                for line in f:\n",
    "                    line.rstrip()\n",
    "                    data.append(line)\n",
    "                    files.append(filename)\n",
    "                    target.append(label)\n",
    "    if encoding is not None:\n",
    "        data = [d.decode(encoding, decode_error) for d in data]\n",
    "        \n",
    "    return Bunch(data=data,\n",
    "                    filenames=files,\n",
    "                    target_names=target_names,\n",
    "                    target=target,\n",
    "                    DESCR=description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis of Sentence in Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import sklearn\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# loading all files as training data. \n",
    "datadir = r'data'\n",
    "data_train = load_sentence_files(datadir, encoding='utf-8')\n",
    "\n",
    "# creat instance of CountVectorizer using MeCab\n",
    "wd = WordDividor('ipadic')\n",
    "data_vec = CountVectorizer(min_df=1, analyzer=wd.extract_words)\n",
    "\n",
    "# split to train and test\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    data_train.data, data_train.target, random_state = 12)\n",
    "\n",
    "text_clf = Pipeline([('vect', data_vec),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())\n",
    "                     ])\n",
    "# Train a Multimoda Naive Bayes classifier\n",
    "text_clf = text_clf.fit(docs_train, y_train)\n",
    "# Predicting the Test set results, find accuracy\n",
    "#y_pred = clf.predict(docs_test)\n",
    "#sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "pred_data = ['日本人は祖国に帰れ。', '日本人は祖国へ帰れ。','日本人は日本に帰れ。', '私は、無関心です。']\n",
    "#pred = text_clf.predict_proba(pred_data)[:, 0]\n",
    "pred = text_clf.predict(pred_data)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
